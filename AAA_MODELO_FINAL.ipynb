{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f070b09e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1. PREPARACIÓN DE DATOS ---\n",
      "--- 2. LIMPIEZA DE DATOS ---\n",
      "\n",
      "--- 3. CREACIÓN DE SPLITS Y NORMALIZACIÓN ---\n",
      "\n",
      "--- 4. DEFINICIÓN DEL MODELO BASE (EL PROFESOR) ---\n",
      "\n",
      "--- 5. FASE 1: ENTRENAMIENTO INICIAL ---\n",
      "El profesor ha estudiado los datos etiquetados.\n",
      "\n",
      "--- 6. FASE 2: PSEUDO-LABELING (EL TRUCO) ---\n",
      "Pocas muestras seguras. Bajando exigencia al 80%...\n",
      "¡Encontradas 57 muestras en el Test con confianza > 80.0%!\n",
      "Dataset original: 1002 muestras.\n",
      "Dataset aumentado: 1059 muestras.\n",
      "\n",
      "--- 7. FASE 3: RE-ENTRENAMIENTO FINAL ---\n",
      "El modelo ha sido re-entrenado con éxito (Semi-Supervised Learning).\n",
      "\n",
      "--- 8. GUARDANDO RESULTADOS ---\n",
      "CSV de submission creado con éxito.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "\n",
    "\n",
    "print(\"--- 1. PREPARACIÓN DE DATOS ---\")\n",
    "\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv') # Datos sin etiqueta (el examen)\n",
    "df_statlog = pd.read_csv('statlog_limpio.csv') # Datos externos\n",
    "\n",
    "\n",
    "print(\"--- 2. LIMPIEZA DE DATOS ---\")\n",
    "\n",
    "# =====================================================================\n",
    "# 1 OLDPEAK (limpiar valores negativos)\n",
    "# =====================================================================\n",
    "# Convertir oldpeak a numérico, forzando errores a NaN\n",
    "train['oldpeak'] = pd.to_numeric(train['oldpeak'], errors='coerce')\n",
    "test['oldpeak']  = pd.to_numeric(test['oldpeak'], errors='coerce')\n",
    "# Convertir valores negativos en NaN\n",
    "train.loc[train['oldpeak'] < 0, 'oldpeak'] = np.nan\n",
    "test.loc[test['oldpeak'] < 0, 'oldpeak'] = np.nan\n",
    "\n",
    "# =====================================================================\n",
    "# 2 DEFINIR FUNCIÓN HOSPITAL\n",
    "# =====================================================================\n",
    "def asignar_hospital(fila):\n",
    "    fila_str = fila.astype(str).str.strip()\n",
    "\n",
    "    # Hospital A: cualquier -9 (que coincide con floats)\n",
    "    if ('-9' in fila_str.values) or ('-9.0' in fila_str.values):\n",
    "        return \"Hospital A\"\n",
    "\n",
    "    # Hospital B: tiene ? (que coincide con ints)\n",
    "    elif ('?' in fila_str.values):\n",
    "        return \"Hospital B\"\n",
    "\n",
    "    else:\n",
    "        return \"Hospital A\"\n",
    "\n",
    "train['hospital'] = train.apply(asignar_hospital, axis=1)\n",
    "test['hospital'] = test.apply(asignar_hospital, axis=1)\n",
    "# Pasar a 0/1\n",
    "train['hospital'] = train['hospital'].str.lower().str.strip().map(lambda x: 1 if 'b' in x else 0)\n",
    "test['hospital']  = test['hospital'].str.lower().str.strip().map(lambda x: 1 if 'b' in x else 0)\n",
    "\n",
    "\n",
    "# =====================================================================\n",
    "# 3 ELIMINAR COLUMNAS QUE EMPEORAN EL MODELO\n",
    "# =====================================================================\n",
    "for df in [train, test]:\n",
    "    if 'ca' in df.columns:\n",
    "        df.drop(columns=['ca'], inplace=True)\n",
    "for df in [train, test]:\n",
    "    if 'chol' in df.columns:\n",
    "        df.drop(columns=['chol'], inplace=True)\n",
    "\n",
    "\n",
    "# =====================================================================\n",
    "# 4 TRATAMIENTO DE SLOPE Y THAL\n",
    "# =====================================================================\n",
    "for df in [train, test]:\n",
    "    # slope: '1' → '2' (solo si es string '1', no tocar 1.0)\n",
    "    df['slope'] = df['slope'].replace('1', '2')\n",
    "\n",
    "    # thal: '3' → '7' (solo si es string '3', no tocar 3.0)\n",
    "    df['thal'] = df['thal'].replace('3', '7')\n",
    "\n",
    "\n",
    "for df in [train, test]:\n",
    "    df['slope'] = pd.to_numeric(df['slope'], errors='coerce')\n",
    "    df['thal']  = pd.to_numeric(df['thal'], errors='coerce')\n",
    "\n",
    "# Regla 1: slope == -9 → slope = 1\n",
    "train.loc[train['slope'] == -9, 'slope'] = 1\n",
    "test.loc[test['slope'] == -9, 'slope'] = 1\n",
    "\n",
    "# Regla 2: slope = 1 y thal = -9 → thal = 3\n",
    "mask_train = (train['slope'] == 1) & (train['thal'] == -9)\n",
    "mask_test  = (test['slope'] == 1) & (test['thal'] == -9)\n",
    "\n",
    "train.loc[mask_train, 'thal'] = 3\n",
    "test.loc[mask_test, 'thal'] = 3\n",
    "\n",
    "# Regla 3: thal = -9 y slope != 1 → thal = 7\n",
    "mask_train2 = (train['thal'] == -9) & (train['slope'] != 1)\n",
    "mask_test2  = (test['thal'] == -9) & (test['slope'] != 1)\n",
    "\n",
    "train.loc[mask_train2, 'thal'] = 7\n",
    "test.loc[mask_test2, 'thal'] = 7\n",
    "\n",
    "\n",
    "# =====================================================================\n",
    "# 6 CONVERTIR -9, ? a nan\n",
    "# =====================================================================\n",
    "for df in [train, test]:\n",
    "\n",
    "    # Convertir texto a numérico cuando posible\n",
    "    for col in df.columns:\n",
    "        df[col] = pd.to_numeric(df[col], errors='ignore')\n",
    "\n",
    "    # Reemplazar -9, '?'\n",
    "    df.replace('-9.0', np.nan, inplace=True)\n",
    "    df.replace('?', np.nan, inplace=True)\n",
    "\n",
    "\n",
    "# =====================================================================\n",
    "# 7 RELLENAR NAN CON MEDIANA DE TRAIN\n",
    "# =====================================================================\n",
    "medianas = train.median(numeric_only=True)\n",
    "train.fillna(medianas, inplace=True)\n",
    "test.fillna(medianas, inplace=True)\n",
    "\n",
    "\n",
    "# =====================================================================\n",
    "# 8 FUSIONAR CATEGORÍAS DE RESTECG: 0 = normal, 1 = anormal\n",
    "# =====================================================================\n",
    "# Asegurar que restecg sea numérico en train y test\n",
    "train['restecg'] = pd.to_numeric(train['restecg'], errors='coerce')\n",
    "test['restecg']  = pd.to_numeric(test['restecg'], errors='coerce')\n",
    "\n",
    "# Fusionar categorías de restecg\n",
    "train['restecg'] = train['restecg'].replace({1: 'anormal', 2: 'anormal'})\n",
    "train['restecg'] = train['restecg'].replace({0: 'normal'})\n",
    "# Codificar a 0 y 1\n",
    "train['restecg'] = train['restecg'].map({'normal': 0, 'anormal': 1})\n",
    "\n",
    "#lo mismo con test\n",
    "test['restecg'] = test['restecg'].replace({1: 'anormal', 2: 'anormal'})\n",
    "test['restecg'] = test['restecg'].replace({0: 'normal'})\n",
    "test['restecg'] = test['restecg'].map({'normal': 0, 'anormal': 1})\n",
    "\n",
    "\n",
    "# =====================================================================\n",
    "# 9 AÑADIR NUEVAS MUESTRAS \n",
    "# =====================================================================\n",
    "# FUSIÓN: Unimos train y statlog para tener la base de conocimiento inicial\n",
    "# Añadimos la variable hospital y asignamos hospital = 0 a todas las nuevas muestras\n",
    "if 'hospital' not in df_statlog.columns:\n",
    "    df_statlog['hospital'] = 0  \n",
    "\n",
    "# Eliminar las mismas columnas que en el train \n",
    "if 'ca' in df_statlog.columns:\n",
    "    df_statlog.drop(columns=['ca'], inplace=True)\n",
    "if 'chol' in df_statlog.columns:\n",
    "    df_statlog.drop(columns=['chol'], inplace=True)\n",
    "\n",
    "# Ahora podemos concatenar\n",
    "cols_train = train.columns\n",
    "df_train_full = pd.concat([train, df_statlog], axis=0, ignore_index=True)\n",
    "df_test = test\n",
    "\n",
    "\n",
    "# =====================================================================\n",
    "# 10 LIMPIAR EL DF FUSIONADO \n",
    "# =====================================================================\n",
    "# Quitar IDs y convertimos a numerico todo el df\n",
    "for col in df_train_full.columns:\n",
    "    if 'id' in col.lower() or 'patient' in col.lower():\n",
    "        df_train_full.drop(columns=[col], inplace=True)\n",
    "        if col in df_test.columns:\n",
    "            df_test.drop(columns=[col], inplace=True)\n",
    "    \n",
    "    df_train_full[col] = pd.to_numeric(df_train_full[col], errors='coerce')\n",
    "    if col in df_test.columns:\n",
    "        df_test[col] = pd.to_numeric(df_test[col], errors='coerce')\n",
    "\n",
    "# Rellenar con mediana por si queda algun nulo\n",
    "medians = df_train_full.median()\n",
    "df_train_full.fillna(medians, inplace=True)\n",
    "df_test.fillna(medians, inplace=True)\n",
    "\n",
    "\n",
    "print(\"\\n--- 3. CREACIÓN DE SPLITS Y NORMALIZACIÓN ---\")\n",
    "\n",
    "# SEPARACIÓN X e y\n",
    "target_col = 'label'\n",
    "X = df_train_full.drop(columns=[target_col])\n",
    "y = df_train_full[target_col]\n",
    "# Alinear test\n",
    "X_test_final = df_test[X.columns]\n",
    "# ESCALADO\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_test_scaled = scaler.transform(X_test_final)\n",
    "\n",
    "\n",
    "print(\"\\n--- 4. DEFINICIÓN DEL MODELO BASE (EL PROFESOR) ---\")\n",
    "\n",
    "# Usamos nuestro mejor Ensemble: Regresión Logística + Random Forest\n",
    "clf1 = LogisticRegression(solver='lbfgs', multi_class='multinomial', max_iter=5000, C=1.0, random_state=42)\n",
    "clf2 = RandomForestClassifier(n_estimators=300, max_depth=12, random_state=42)\n",
    "model = VotingClassifier(\n",
    "    estimators=[('lr', clf1), ('rf', clf2)],\n",
    "    voting='soft' # Importante: 'soft' para obtener probabilidades\n",
    ")\n",
    "\n",
    "\n",
    "print(\"\\n--- 5. FASE 1: ENTRENAMIENTO INICIAL ---\")\n",
    "\n",
    "model.fit(X_scaled, y)\n",
    "print(\"El profesor ha estudiado los datos etiquetados.\")\n",
    "\n",
    "\n",
    "print(\"\\n--- 6. FASE 2: PSEUDO-LABELING (EL TRUCO) ---\")\n",
    "\n",
    "# El modelo predice las PROBABILIDADES sobre el test\n",
    "probs = model.predict_proba(X_test_scaled)\n",
    "preds = model.predict(X_test_scaled)\n",
    "# Buscamos muestras con confianza ALTA\n",
    "threshold = 0.90 # 90% de seguridad\n",
    "high_conf_indices = np.where(np.max(probs, axis=1) > threshold)[0]\n",
    "# Si hay pocas, bajamos un poco la vara para no quedarnos sin datos extra\n",
    "if len(high_conf_indices) < 50:\n",
    "    print(f\"Pocas muestras seguras. Bajando exigencia al 80%...\")\n",
    "    threshold = 0.80\n",
    "    high_conf_indices = np.where(np.max(probs, axis=1) > threshold)[0]\n",
    "print(f\"¡Encontradas {len(high_conf_indices)} muestras en el Test con confianza > {threshold*100}%!\")\n",
    "\n",
    "# Extraemos esas muestras y sus predicciones (que ahora son sus 'etiquetas')\n",
    "X_pseudo_scaled = X_test_scaled[high_conf_indices]\n",
    "y_pseudo = preds[high_conf_indices]\n",
    "# AUMENTO DE DATOS\n",
    "# Unimos los datos originales (X_scaled) con los nuevos datos pseudo-etiquetados (X_pseudo_scaled)\n",
    "X_augmented = np.vstack((X_scaled, X_pseudo_scaled))\n",
    "y_augmented = np.concatenate((y, y_pseudo))\n",
    "print(f\"Dataset original: {X_scaled.shape[0]} muestras.\")\n",
    "print(f\"Dataset aumentado: {X_augmented.shape[0]} muestras.\")\n",
    "\n",
    "\n",
    "print(\"\\n--- 7. FASE 3: RE-ENTRENAMIENTO FINAL ---\")\n",
    "\n",
    "# El modelo vuelve a estudiar, ahora con más material (incluyendo lo que \"aprendió\" del test)\n",
    "model.fit(X_augmented, y_augmented)\n",
    "print(\"El modelo ha sido re-entrenado con éxito (Semi-Supervised Learning).\")\n",
    "# Aquí terminaría la lógica del modelo.\n",
    "# El siguiente paso sería predecir de nuevo sobre X_test_scaled para generar el submission.\n",
    "# Predicciones finales sobre todo el test\n",
    "# Predicciones finales sobre todo el test\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "\n",
    "print(\"\\n--- 8. GUARDANDO RESULTADOS ---\")\n",
    "\n",
    "# Crear submission con índice como ID\n",
    "submission = pd.DataFrame({\n",
    "    'ID': range(len(y_pred)),  # ID consecutivo 0,1,2,...\n",
    "    'label': y_pred\n",
    "})\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "print(\"CSV de submission creado con éxito.\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datos_no_estructurados",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
